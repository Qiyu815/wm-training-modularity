{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_logfile(fname: str, task_meta: dict) -> pd.DataFrame:\n",
    "    '''Converts logfile into concise and easy to work DataFrame\n",
    "    \n",
    "    Args:\n",
    "        fname: path to log file\n",
    "        task_meta: task metadata\n",
    "        \n",
    "    Returns:\n",
    "        Table containing information about each trial. Each row contain block\n",
    "        number, task condition, correct and actual response for both visual and\n",
    "        auditory stimuli.    \n",
    "    '''\n",
    "    # Setup local variables\n",
    "    n_stims = task_meta['n_stims']\n",
    "    n_blocks = task_meta['n_blocks']\n",
    "    n_conditions = task_meta['n_conditions']\n",
    "    n_trials = n_stims * n_blocks * n_conditions\n",
    "    ttr = task_meta['ttr']\n",
    "    \n",
    "    # Create block and condition indices\n",
    "    ind_block = np.repeat(np.arange(n_blocks), n_stims * n_conditions)[:, np.newaxis]\n",
    "    ind_cond = np.tile(\n",
    "        np.concatenate((np.ones(n_stims), 2*np.ones(n_stims))), n_blocks\n",
    "    )[:, np.newaxis]\n",
    "\n",
    "    df_ind = pd.DataFrame(\n",
    "        np.hstack((ind_block, ind_cond)), \n",
    "        columns=['block', 'condition']).astype('int')\n",
    "\n",
    "    # Load stimulus data\n",
    "    df = pd.read_csv(fname, delimiter='\\t', skiprows=3)\n",
    "\n",
    "    stim_filter = df['Code'].str.contains('yes') | df['Code'].str.contains('no')\n",
    "    df_stim = df.loc[stim_filter, :]\n",
    "\n",
    "    # Split visual and audio stimuli\n",
    "    df_stim_vis = df_stim.loc[df_stim['Event Type'] == 'Picture',  \n",
    "                              ['xs(str)', 'Time']].reset_index(drop=True)\n",
    "    df_stim_aud = df_stim.loc[df_stim['Event Type'] == 'Sound',  \n",
    "                              'xs(str)'].reset_index(drop=True)\n",
    "    df_stim_vis.columns = ['ans_vis', 'stim_onset']\n",
    "    df_stim_aud.name = 'ans_aud'\n",
    "\n",
    "    # Merge visual and audio stimuli again & add response columns\n",
    "    df_stim = pd.concat([df_ind, df_stim_vis, df_stim_aud], axis=1, sort=False)\n",
    "    df_stim['resp_aud'], df_stim['resp_aud_time'], \\\n",
    "    df_stim['resp_vis'], df_stim['resp_vis_time'] = np.zeros((4, n_trials), dtype='int')\n",
    "    \n",
    "    # Load response data\n",
    "    resp_filer = df['Code'].isin(['1', '2'])\n",
    "    df_resp = df.loc[resp_filer, :]\n",
    "\n",
    "    # Analyze trialwise responses\n",
    "    for i, row in df_stim.iterrows():\n",
    "\n",
    "        r = df_resp.loc[(df_resp['Time'] > row['stim_onset']) & \\\n",
    "                        (df_resp['Time'] < row['stim_onset'] + ttr), \n",
    "                        ['Code', 'Time']]\n",
    "\n",
    "        if '2' in r['Code'].unique():\n",
    "            df_stim.loc[i, 'resp_aud'] = 1\n",
    "            df_stim.loc[i, 'resp_aud_time'] = r.loc[r['Code']=='2', 'Time'].values[0] \n",
    "        if '1' in r['Code'].unique():\n",
    "            df_stim.loc[i, 'resp_vis'] = 1\n",
    "            df_stim.loc[i, 'resp_vis_time'] = r.loc[r['Code']=='1', 'Time'].values[0] \n",
    "\n",
    "    for modality in ['vis', 'aud']:\n",
    "        df_stim[f'resp_{modality}_time'] -= df_stim['stim_onset'] \n",
    "        df_stim.loc[df_stim[f'resp_{modality}_time'] < 0, \n",
    "                    f'resp_{modality}_time'] = np.nan\n",
    "        df_stim[f'resp_{modality}_time'] /= 10\n",
    "        df_stim[f'ans_{modality}'] = df_stim[f'ans_{modality}'].map({' yes': 1, ' no': 0})\n",
    "\n",
    "    # Drop and reorder columns\n",
    "    df_stim = df_stim[['block', 'condition', \n",
    "                       'ans_vis', 'resp_vis', 'resp_vis_time',\n",
    "                       'ans_aud', 'resp_aud', 'resp_aud_time']]\n",
    "    \n",
    "    return df_stim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral measures\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We assume that each measure is calculated separately for single subject, session, task condition (1-back or 2-back) and stimuli modality (visual or auditory). Each behavioral response can be divided into one of four categories: *hit*, *miss*, *correct rejection* or *false alarm*:\n",
    "\n",
    "| Stimuli\\Response \t| Congruent   \t| Incongruent       \t|\n",
    "|------------------\t|-------------\t|-------------------\t|\n",
    "| Congruent        \t| hit         \t| miss              \t|\n",
    "| Incongruent      \t| false alarm \t| correct rejection \t|\n",
    "\n",
    "Subjects were instructed to respond with their thumb to congruent stimuli and omit response to incongruent stimuli. Note that in this task setting \"real misses\", i.e. when subject failed to respond within required time window cannot be distinguished from correct rejections and misses. Number of responses of each type was calculated on the level of entire run and for individual task blocks. Reaction time (RT) was calculated for hits and false alarms.\n",
    "\n",
    "## Measures\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "Accuracy was calculated as the actual number of hits divided by the maximum number of hits.\n",
    "$$accuracy = \\frac{\\#hits}{N_{congruent}}$$\n",
    "\n",
    "This imperfect definition is prone to *respond to all stimuli* behavior. In this case accuracy will be one.\n",
    "\n",
    "### Penalized reaction time\n",
    "\n",
    "Penalized reaction time (PRT) tries to combine accuracy with the reaction time. Calculating PRT consists of three steps. First, all labeled reaction times are selected. Second, all reaction times for false alarms are set to 2000ms (maximal possible RT). Third, fake reaction times for misses are added to penalize for not responding. Then mean is calculated for task runs and individual blocks. Note that mean PRT over blocks is usually different than the one calculated for the enire task mean (some blocks consist of more responses than others). Formally, PRT is defined as\n",
    "$$prt = \\frac{1}{\\#false\\ alarms + \\#hits + \\#misses}(\\sum_{r=1}^{N_{responses}}  prt_{r} + \\#misses \\times 2000\\text{ms})$$\n",
    "where $prt_{r}$ is 2000ms for false alarms and RT for hits.\n",
    "\n",
    "### D-prime\n",
    "\n",
    "D-prime based on signal detection theory takes into account both response sensitivity and specificity. First, *hit rate* $hr$ and *false alarm rate* $fr$ is calculated as:\n",
    "$$hr = \\frac{\\#hits}{\\#hits + \\#misses}$$\n",
    "$$fr = \\frac{\\#false\\ alarms}{\\#false\\ alarms + \\#correct\\ rejections}$$\n",
    "Second, to avoid problems with infinite values, all $hr$ or $fr$ values equal to 1 are set to 0.99 and all $hr$ or $fr$ values equal to 0 are set to 0.01. Finally, d-prime is calculated as difference between pdf transormed values of $hr$ and $fr$:\n",
    "$$d'=f(hr)-f(fr)$$\n",
    "where \n",
    "$$f(x)=\\frac{\\exp (-x^2/2)}{\\sqrt{2\\pi}}$$\n",
    "Note that mean d-prime over task blocks is not equal to mean calculated for the entire task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_behavioral_measures(df_stim: pd.DataFrame, task_meta: dict) -> tuple:\n",
    "    '''Calculates main three behavioral measures: accuracy,  pRT and D-prime.\n",
    "    \n",
    "    Args: \n",
    "        df_stim: trialwise task information\n",
    "        task_meta: task_meta\n",
    "        \n",
    "    Returns:\n",
    "        Aggregated behavioral measures for entire task. \n",
    "            (n_measures x n_conditions x n_modalities) \n",
    "        Aggregated behavioral measures for specific blocks.\n",
    "            (n_measures x n_conditions x n_modalities x n_blocks)\n",
    "    '''\n",
    "\n",
    "    def _get_resp_patterns(df_signal, filt_rows, modality):\n",
    "        '''Calculates number of hits, correct rejections, misses and false alarms.\n",
    "\n",
    "        Args: \n",
    "            modality (str): either 'vis' or 'aud'\n",
    "\n",
    "        Returns:\n",
    "            Number of hits, correct rejections, misses and false alarms.\n",
    "\n",
    "        '''\n",
    "        no_hit = df_signal.loc[filt_rows, f'hit_{modality}'].sum()\n",
    "        no_crr = df_signal.loc[filt_rows, f'crr_{modality}'].sum()\n",
    "        no_mis = df_signal.loc[filt_rows, f'mis_{modality}'].sum()\n",
    "        no_fal = df_signal.loc[filt_rows, f'fal_{modality}'].sum()\n",
    "\n",
    "        return (no_hit, no_crr, no_mis, no_fal)\n",
    "\n",
    "    def _calculate_dprime(no_hit, no_crr, no_mis, no_fal):\n",
    "        '''Calculates d-prime signal detection index\n",
    "\n",
    "        Args: \n",
    "            no_hit (int): number of hits\n",
    "            no_crr (int): number of correct rejections\n",
    "            no_mis (int): number of misses\n",
    "            no_fal (int): number of false alarms\n",
    "\n",
    "        Returns:\n",
    "            (float) D-prime index.    \n",
    "        '''\n",
    "\n",
    "        hit_rate = no_hit / (no_hit + no_mis)\n",
    "        fal_rate = no_fal / (no_fal + no_crr)\n",
    "\n",
    "        # Corner cases (infinity problem)\n",
    "        if fal_rate == 0: fal_rate = 0.01\n",
    "        if fal_rate == 1: fal_rate = 0.99\n",
    "\n",
    "        if hit_rate == 0: hit_rate = 0.01\n",
    "        if hit_rate == 1: hit_rate = 0.99\n",
    "\n",
    "        return norm.ppf(hit_rate) - norm.ppf(fal_rate)\n",
    "\n",
    "    df_signal = df_stim[['block', 'condition']].copy()\n",
    "    n_modalities = task_meta['n_modalities']\n",
    "    n_conditions = task_meta['n_conditions']\n",
    "    n_blocks = task_meta['n_blocks']\n",
    "    ttr = task_meta['ttr'] / 10\n",
    "\n",
    "    # Signal theory measures\n",
    "    for s in ['vis', 'aud']:\n",
    "        df_signal[f'hit_{s}'] = (df_stim[f'ans_{s}'] == 1) & (df_stim[f'resp_{s}'] == 1)\n",
    "        df_signal[f'crr_{s}'] = (df_stim[f'ans_{s}'] == 0) & (df_stim[f'resp_{s}'] == 0)\n",
    "        df_signal[f'mis_{s}'] = (df_stim[f'ans_{s}'] == 1) & (df_stim[f'resp_{s}'] == 0)\n",
    "        df_signal[f'fal_{s}'] = (df_stim[f'ans_{s}'] == 0) & (df_stim[f'resp_{s}'] == 1)\n",
    "        df_signal[f'hit_time_{s}'] = df_signal[f'hit_{s}'].replace(False, np.nan) \\\n",
    "                                   * df_stim[f'resp_{s}_time']\n",
    "        df_signal[f'fal_time_{s}'] = df_signal[f'fal_{s}'].replace(False, np.nan) \\\n",
    "                                   * df_stim[f'resp_{s}_time']\n",
    "\n",
    "    acc_block, prt_block, dpr_block = np.zeros((3, n_conditions, n_modalities, n_blocks))\n",
    "    acc, prt, dpr = np.zeros((3, n_conditions, n_modalities))\n",
    "\n",
    "    for idx_c, condition in enumerate([1, 2]):\n",
    "        for idx_m, modality in enumerate(['vis', 'aud']):\n",
    "\n",
    "            filt_rows = (df_signal['condition'] == condition)\n",
    "\n",
    "            # Count hits, correct rejections, misses and false alarms\n",
    "            no_hit, no_crr, no_mis, no_fal = _get_resp_patterns(df_signal, filt_rows, modality)\n",
    "\n",
    "            # Calculate behavioral measures (whole task)\n",
    "            prt[idx_c, idx_m] = \\\n",
    "                (df_signal.loc[filt_rows, f'hit_time_{modality}'].sum() + ttr * no_fal) \\\n",
    "              / (no_hit + no_fal)\n",
    "            dpr[idx_c, idx_m] = _calculate_dprime(no_hit, no_crr, no_mis, no_fal)\n",
    "\n",
    "            for idx_b in range(n_blocks):\n",
    "\n",
    "                filt_rows = (df_signal['condition'] == condition) \\\n",
    "                          & (df_signal['block'] == idx_b)\n",
    "\n",
    "                # Count hits, correct rejections, misses and false alarms\n",
    "                no_hit, no_crr, no_mis, no_fal = _get_resp_patterns(\n",
    "                    df_signal, filt_rows, modality\n",
    "                )\n",
    "\n",
    "                # Calculate behavioral measures (block level)\n",
    "                acc_block[idx_c, idx_m, idx_b] = no_hit / (no_hit + no_mis)\n",
    "                prt_block[idx_c, idx_m, idx_b] = \\\n",
    "                    (df_signal.loc[filt_rows, f'hit_time_{modality}'].sum() + ttr * (no_fal + no_mis)) \\\n",
    "                  / (no_hit + no_fal + no_mis)\n",
    "                dpr_block[idx_c, idx_m, idx_b] = _calculate_dprime(no_hit, no_crr, no_mis, no_fal)\n",
    "\n",
    "    acc = np.nanmean(acc_block, axis=2)\n",
    "    \n",
    "    return (np.stack((acc, prt, dpr)), np.stack((acc_block, prt_block, dpr_block)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load group assignment table and exclude subjects with incomplete data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>sub-01</td>\n",
       "      <td>Control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>sub-02</td>\n",
       "      <td>Control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>sub-04</td>\n",
       "      <td>Control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>sub-05</td>\n",
       "      <td>Experimental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>sub-06</td>\n",
       "      <td>Experimental</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sub         group\n",
       "0  sub-01       Control\n",
       "1  sub-02       Control\n",
       "3  sub-04       Control\n",
       "4  sub-05  Experimental\n",
       "5  sub-06  Experimental"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_group = pd.read_csv('data/group_assignment.csv')\n",
    "df_group = df_group.loc[~df_group['group'].isna(), :]\n",
    "df_group.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process all logs and save aggregated behavioral data to file. Data description:\n",
    "- `beh`: np.array containing behavioral measures calculated for entire dual n-back run\n",
    "- `beh_block`: contains the same measures calculated for individual task blocks\n",
    "- `meta`: dictionary describing fields in `beh` and `beh_block`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs_dual/001d_1-dual_n-back_modified.log\n",
      "logs_dual/002d_1-dual_n-back_modified.log\n",
      "logs_dual/004d_1-dual_n-back_modified.log\n",
      "logs_dual/005d_1-dual_n-back_modified.log\n",
      "logs_dual/006d_1-dual_n-back_modified.log\n",
      "logs_dual/007d_1-dual_n-back_modified.log\n",
      "logs_dual/008d_1-dual_n-back_modified.log\n",
      "logs_dual/010d_1-dual_n-back_modified.log\n",
      "logs_dual/011d_1-dual_n-back_modified.log\n",
      "logs_dual/012d_1-dual_n-back_modified.log\n",
      "logs_dual/013d_1-dual_n-back_modified.log\n",
      "logs_dual/014d_1-dual_n-back_modified.log\n",
      "logs_dual/015d_1-dual_n-back_modified.log\n",
      "logs_dual/016d_1-dual_n-back_modified.log\n",
      "logs_dual/018d_1-dual_n-back_modified.log\n",
      "logs_dual/019d_1-dual_n-back_modified.log\n",
      "logs_dual/020d_1-dual_n-back_modified.log\n",
      "logs_dual/021d_1-dual_n-back_modified.log\n",
      "logs_dual/023d_1-dual_n-back_modified.log\n",
      "logs_dual/024d_1-dual_n-back_modified.log\n",
      "logs_dual/025d_1-dual_n-back_modified.log\n",
      "logs_dual/026d_1-dual_n-back_modified.log\n",
      "logs_dual/027d_1-dual_n-back_modified.log\n",
      "logs_dual/028d_1-dual_n-back_modified.log\n",
      "logs_dual/029d_1-dual_n-back_modified.log\n",
      "logs_dual/030d_1-dual_n-back_modified.log\n",
      "logs_dual/033d_1-dual_n-back_modified.log\n",
      "logs_dual/035d_1-dual_n-back_modified.log\n",
      "logs_dual/036d_1-dual_n-back_modified.log\n",
      "logs_dual/037d_1-dual_n-back_modified.log\n",
      "logs_dual/039d_1-dual_n-back_modified.log\n",
      "logs_dual/041d_1-dual_n-back_modified.log\n",
      "logs_dual/042d_1-dual_n-back_modified.log\n",
      "logs_dual/043d_1-dual_n-back_modified.log\n",
      "logs_dual/044d_1-dual_n-back_modified.log\n",
      "logs_dual/045d_1-dual_n-back_modified.log\n",
      "logs_dual/046d_1-dual_n-back_modified.log\n",
      "logs_dual/047d_1-dual_n-back_modified.log\n",
      "logs_dual/048d_1-dual_n-back_modified.log\n",
      "logs_dual/050d_1-dual_n-back_modified.log\n",
      "logs_dual/051d_1-dual_n-back_modified.log\n",
      "logs_dual/052d_1-dual_n-back_modified.log\n",
      "logs_dual/053d_1-dual_n-back_modified.log\n",
      "logs_dual/054d_1-dual_n-back_modified.log\n",
      "logs_dual/055d_1-dual_n-back_modified.log\n",
      "logs_dual/056d_1-dual_n-back_modified.log\n",
      "logs_dual/001d_2-dual_n-back_modified.log\n",
      "logs_dual/002d_2-dual_n-back_modified.log\n",
      "logs_dual/004d_2-dual_n-back_modified.log\n",
      "logs_dual/005d_2-dual_n-back_modified.log\n",
      "logs_dual/006d_2-dual_n-back_modified.log\n",
      "logs_dual/007d_2-dual_n-back_modified.log\n",
      "logs_dual/008d_2-dual_n-back_modified.log\n",
      "logs_dual/010d_2-dual_n-back_modified.log\n",
      "logs_dual/011d_2-dual_n-back_modified.log\n",
      "logs_dual/012d_2-dual_n-back_modified.log\n",
      "logs_dual/013d_2-dual_n-back_modified.log\n",
      "logs_dual/014d_2-dual_n-back_modified.log\n",
      "logs_dual/015d_2-dual_n-back_modified.log\n",
      "logs_dual/016d_2-dual_n-back_modified.log\n",
      "logs_dual/018d_2-dual_n-back_modified.log\n",
      "logs_dual/019d_2-dual_n-back_modified.log\n",
      "logs_dual/020d_2-dual_n-back_modified.log\n",
      "logs_dual/021d_2-dual_n-back_modified.log\n",
      "logs_dual/023d_2-dual_n-back_modified.log\n",
      "logs_dual/024d_2-dual_n-back_modified.log\n",
      "logs_dual/025d_2-dual_n-back_modified.log\n",
      "logs_dual/026d_2-dual_n-back_modified.log\n",
      "logs_dual/027d_2-dual_n-back_modified.log\n",
      "logs_dual/028d_2-dual_n-back_modified.log\n",
      "logs_dual/029d_2-dual_n-back_modified.log\n",
      "logs_dual/030d_2-dual_n-back_modified.log\n",
      "logs_dual/033d_2-dual_n-back_modified.log\n",
      "logs_dual/035d_2-dual_n-back_modified.log\n",
      "logs_dual/036d_2-dual_n-back_modified.log\n",
      "logs_dual/037d_2-dual_n-back_modified.log\n",
      "logs_dual/039d_2-dual_n-back_modified.log\n",
      "logs_dual/041d_2-dual_n-back_modified.log\n",
      "logs_dual/042d_2-dual_n-back_modified.log\n",
      "logs_dual/043d_2-dual_n-back_modified.log\n",
      "logs_dual/044d_2-dual_n-back_modified.log\n",
      "logs_dual/045d_2-dual_n-back_modified.log\n",
      "logs_dual/046d_2-dual_n-back_modified.log\n",
      "logs_dual/047d_2-dual_n-back_modified.log\n",
      "logs_dual/048d_2-dual_n-back_modified.log\n",
      "logs_dual/050d_2-dual_n-back_modified.log\n",
      "logs_dual/051d_2-dual_n-back_modified.log\n",
      "logs_dual/052d_2-dual_n-back_modified.log\n",
      "logs_dual/053d_2-dual_n-back_modified.log\n",
      "logs_dual/054d_2-dual_n-back_modified.log\n",
      "logs_dual/055d_2-dual_n-back_modified.log\n",
      "logs_dual/056d_2-dual_n-back_modified.log\n",
      "logs_dual/001d_3-dual_n-back_modified.log\n",
      "logs_dual/002d_3-dual_n-back_modified.log\n",
      "logs_dual/004d_3-dual_n-back_modified.log\n",
      "logs_dual/005d_3-dual_n-back_modified.log\n",
      "logs_dual/006d_3-dual_n-back_modified.log\n",
      "logs_dual/007d_3-dual_n-back_modified.log\n",
      "logs_dual/008d_3-dual_n-back_modified.log\n",
      "logs_dual/010d_3-dual_n-back_modified.log\n",
      "logs_dual/011d_3-dual_n-back_modified.log\n",
      "logs_dual/012d_3-dual_n-back_modified.log\n",
      "logs_dual/013d_3-dual_n-back_modified.log\n",
      "logs_dual/014d_3-dual_n-back_modified.log\n",
      "logs_dual/015d_3-dual_n-back_modified.log\n",
      "logs_dual/016d_3-dual_n-back_modified.log\n",
      "logs_dual/018d_3-dual_n-back_modified.log\n",
      "logs_dual/019d_3-dual_n-back_modified.log\n",
      "logs_dual/020d_3-dual_n-back_modified.log\n",
      "logs_dual/021d_3-dual_n-back_modified.log\n",
      "logs_dual/023d_3-dual_n-back_modified.log\n",
      "logs_dual/024d_3-dual_n-back_modified.log\n",
      "logs_dual/025d_3-dual_n-back_modified.log\n",
      "logs_dual/026d_3-dual_n-back_modified.log\n",
      "logs_dual/027d_3-dual_n-back_modified.log\n",
      "logs_dual/028d_3-dual_n-back_modified.log\n",
      "logs_dual/029d_3-dual_n-back_modified.log\n",
      "logs_dual/030d_3-dual_n-back_modified.log\n",
      "logs_dual/033d_3-dual_n-back_modified.log\n",
      "logs_dual/035d_3-dual_n-back_modified.log\n",
      "logs_dual/036d_3-dual_n-back_modified.log\n",
      "logs_dual/037d_3-dual_n-back_modified.log\n",
      "logs_dual/039d_3-dual_n-back_modified.log\n",
      "logs_dual/041d_3-dual_n-back_modified.log\n",
      "logs_dual/042d_3-dual_n-back_modified.log\n",
      "logs_dual/043d_3-dual_n-back_modified.log\n",
      "logs_dual/044d_3-dual_n-back_modified.log\n",
      "logs_dual/045d_3-dual_n-back_modified.log\n",
      "logs_dual/046d_3-dual_n-back_modified.log\n",
      "logs_dual/047d_3-dual_n-back_modified.log\n",
      "logs_dual/048d_3-dual_n-back_modified.log\n",
      "logs_dual/050d_3-dual_n-back_modified.log\n",
      "logs_dual/051d_3-dual_n-back_modified.log\n",
      "logs_dual/052d_3-dual_n-back_modified.log\n",
      "logs_dual/053d_3-dual_n-back_modified.log\n",
      "logs_dual/054d_3-dual_n-back_modified.log\n",
      "logs_dual/055d_3-dual_n-back_modified.log\n",
      "logs_dual/056d_3-dual_n-back_modified.log\n",
      "logs_dual/001d_4-dual_n-back_modified.log\n",
      "logs_dual/002d_4-dual_n-back_modified.log\n",
      "logs_dual/004d_4-dual_n-back_modified.log\n",
      "logs_dual/005d_4-dual_n-back_modified.log\n",
      "logs_dual/006d_4-dual_n-back_modified.log\n",
      "logs_dual/007d_4-dual_n-back_modified.log\n",
      "logs_dual/008d_4-dual_n-back_modified.log\n",
      "logs_dual/010d_4-dual_n-back_modified.log\n",
      "logs_dual/011d_4-dual_n-back_modified.log\n",
      "logs_dual/012d_4-dual_n-back_modified.log\n",
      "logs_dual/013d_4-dual_n-back_modified.log\n",
      "logs_dual/014d_4-dual_n-back_modified.log\n",
      "logs_dual/015d_4-dual_n-back_modified.log\n",
      "logs_dual/016d_4-dual_n-back_modified.log\n",
      "logs_dual/018d_4-dual_n-back_modified.log\n",
      "logs_dual/019d_4-dual_n-back_modified.log\n",
      "logs_dual/020d_4-dual_n-back_modified.log\n",
      "logs_dual/021d_4-dual_n-back_modified.log\n",
      "logs_dual/023d_4-dual_n-back_modified.log\n",
      "logs_dual/024d_4-dual_n-back_modified.log\n",
      "logs_dual/025d_4-dual_n-back_modified.log\n",
      "logs_dual/026d_4-dual_n-back_modified.log\n",
      "logs_dual/027d_4-dual_n-back_modified.log\n",
      "logs_dual/028d_4-dual_n-back_modified.log\n",
      "logs_dual/029d_4-dual_n-back_modified.log\n",
      "logs_dual/030d_4-dual_n-back_modified.log\n",
      "logs_dual/033d_4-dual_n-back_modified.log\n",
      "logs_dual/035d_4-dual_n-back_modified.log\n",
      "logs_dual/036d_4-dual_n-back_modified.log\n",
      "logs_dual/037d_4-dual_n-back_modified.log\n",
      "logs_dual/039d_4-dual_n-back_modified.log\n",
      "logs_dual/041d_4-dual_n-back_modified.log\n",
      "logs_dual/042d_4-dual_n-back_modified.log\n",
      "logs_dual/043d_4-dual_n-back_modified.log\n",
      "logs_dual/044d_4-dual_n-back_modified.log\n",
      "logs_dual/045d_4-dual_n-back_modified.log\n",
      "logs_dual/046d_4-dual_n-back_modified.log\n",
      "logs_dual/047d_4-dual_n-back_modified.log\n",
      "logs_dual/048d_4-dual_n-back_modified.log\n",
      "logs_dual/050d_4-dual_n-back_modified.log\n",
      "logs_dual/051d_4-dual_n-back_modified.log\n",
      "logs_dual/052d_4-dual_n-back_modified.log\n",
      "logs_dual/053d_4-dual_n-back_modified.log\n",
      "logs_dual/054d_4-dual_n-back_modified.log\n",
      "logs_dual/055d_4-dual_n-back_modified.log\n",
      "logs_dual/056d_4-dual_n-back_modified.log\n"
     ]
    }
   ],
   "source": [
    "root = './../../data/sourcedata/behavioral/'\n",
    "\n",
    "task_meta = {\n",
    "    'n_stims': 12,\n",
    "    'n_blocks': 10,\n",
    "    'n_modalities': 2,\n",
    "    'n_conditions': 2,\n",
    "    'n_sessions': 4,\n",
    "    'n_measures': 3,\n",
    "    'ttr': 20000,\n",
    "    'n_subjects': df_group.shape[0]\n",
    "}\n",
    "\n",
    "beh = np.full((task_meta['n_subjects'], task_meta['n_sessions'], \n",
    "               task_meta['n_measures'], task_meta['n_conditions'], \n",
    "               task_meta['n_modalities']), \n",
    "              np.nan)\n",
    "beh_block = np.full((task_meta['n_subjects'], task_meta['n_sessions'], \n",
    "                     task_meta['n_measures'], task_meta['n_conditions'], \n",
    "                     task_meta['n_modalities'], task_meta['n_blocks']), \n",
    "                    np.nan)\n",
    "\n",
    "for ix_ses, ses in enumerate(['1', '2', '3', '4']):\n",
    "    for ix_sub, sub in enumerate(df_group['sub']):\n",
    "        \n",
    "        logpath = os.path.join(root, f'0{sub[-2:]}d_{ses}-dual_n-back_modified.log')\n",
    "        \n",
    "        if os.path.exists(logpath):\n",
    "                beh[ix_sub, ix_ses], beh_block[ix_sub, ix_ses] = \\\n",
    "                    calculate_behavioral_measures(convert_logfile(logpath, task_meta), task_meta)\n",
    "                \n",
    "# Create metadata describing beh and beh_block fields\n",
    "meta = {\n",
    "    'dim1': df_group['sub'].tolist(),\n",
    "    'dim2': [f'ses-{ses}' for ses in range(1, task_meta['n_sessions'] + 1)],\n",
    "    'dim3': ['acc', 'prt', 'dpr'],\n",
    "    'dim4': ['1-back', '2-back'],\n",
    "    'dim5': ['vis', 'aud'],\n",
    "    'dim6': [f'block-{block:02}' for block in range(1, task_meta['n_blocks'] + 1)],\n",
    "    'exp': list(df_group['group'] == 'Experimental'),\n",
    "    'con': list(df_group['group'] == 'Control'),\n",
    "}\n",
    "\n",
    "# Save aggregated behavioral measures\n",
    "np.save('data/aggregated_behavioral_data.npy', beh)\n",
    "np.save('data/aggregated_behavioral_data_block.npy', beh_block)\n",
    "with open('data/aggregated_behavioral_data.json', 'w') as f:\n",
    "    json.dump(meta, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
